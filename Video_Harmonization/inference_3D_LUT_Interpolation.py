import os
import argparse

import albumentations
from albumentations import Resize

import torch
import torch.backends.cudnn as cudnn
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

from model.build_model import build_model
from datasets.build_dataset import dataset_generator

from utils import misc, metrics
import numpy as np

from model.lut_transformation_net import TrilinearInterpolation


def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--workers', type=int, default=1,
                        metavar='N', help='Dataloader threads.')

    parser.add_argument('--batch_size', type=int, default=1,
                        help='You can override model batch size by specify positive number.')

    parser.add_argument('--device', type=str, default='cuda',
                        help="Whether use cuda, 'cuda' or 'cpu'.")

    parser.add_argument('--save_path', type=str, default="./logs",
                        help='Where to save logs and checkpoints.')

    parser.add_argument('--dataset_path', type=str, default="/HYouTube/",
                        help='Dataset path.')

    parser.add_argument('--base_size', type=int, default=256,
                        help='Base size. Resolution of the image input into the Encoder')

    parser.add_argument('--input_size', type=int, default=256,
                        help='Input size. Resolution of the image that want to be generated by the Decoder')

    parser.add_argument('--INR_input_size', type=int, default=256,
                        help='INR input size. Resolution of the image that want to be generated by the Decoder. '
                             'Should be the same as `input_size`')

    parser.add_argument('--INR_MLP_dim', type=int, default=32,
                        help='Number of channels for INR linear layer.')

    parser.add_argument('--LUT_dim', type=int, default=7,
                        help='Dim of the output LUT. Refer to https://ieeexplore.ieee.org/abstract/document/9206076')

    parser.add_argument('--activation', type=str, default='leakyrelu_pe',
                        help='INR activation layer type: leakyrelu_pe, sine')

    parser.add_argument('--pretrained', type=str,
                        default=r'.\pretrained_models\Video_HYouTube_256.pth',
                        help='Pretrained weight path')

    parser.add_argument('--param_factorize_dim', type=int,
                        default=10,
                        help='The intermediate dimensions of the factorization of the predicted MLP parameters. '
                             'Refer to https://arxiv.org/abs/2011.12026')

    parser.add_argument('--embedding_type', type=str,
                        default="CIPS_embed",
                        help='Which embedding_type to use.')

    parser.add_argument('--optim', type=str,
                        default='adamw',
                        help='Which optimizer to use.')

    parser.add_argument('--INRDecode', action="store_false",
                        help='Whether INR decoder. Set it to False if you want to test the baseline '
                             '(https://github.com/SamsungLabs/image_harmonization)')

    parser.add_argument('--isMoreINRInput', action="store_false",
                        help='Whether to cat RGB and mask. See Section 3.4 in the paper.')

    parser.add_argument('--hr_train', action="store_true",
                        help='Whether use hr_train. See section 3.4 in the paper.')

    parser.add_argument('--isFullRes', action="store_true",
                        help='Whether for original resolution. See section 3.4 in the paper.')

    opt = parser.parse_args()

    opt.save_path = misc.increment_path(os.path.join(opt.save_path, "test1"))

    return opt


def select(checkpoint, curr_step, lut_list):
    distance = sum([abs(curr_step - i) for i in checkpoint])
    weight = [distance / abs(curr_step - i + 1e-6) for i in checkpoint]
    normal_weight = [i / sum(weight) for i in weight]
    return sum([i * j for i, j in zip(normal_weight, lut_list)])


"This is the implementation of our 3D LUT interpolation strategy. See Section V-C of our paper for more details."
def inference(val_loader, model, logger, opt):
    current_process = 10
    model.eval()

    # You can change this to your own selected key frames. We will only calculate the 3D LUT of these key frames.
    checkpoint = [0, 5, 10, 15, 19]

    metric_log = {
        'HYouTube': {'Samples': 0, 'MSE': 0, 'fMSE': 0, 'PSNR': 0, 'SSIM': 0},
        'All': {'Samples': 0, 'MSE': 0, 'fMSE': 0, 'PSNR': 0, 'SSIM': 0},
    }

    lut_metric_log = {
        'HYouTube': {'Samples': 0, 'MSE': 0, 'fMSE': 0, 'PSNR': 0, 'SSIM': 0},
        'All': {'Samples': 0, 'MSE': 0, 'fMSE': 0, 'PSNR': 0, 'SSIM': 0},
    }

    all_intermediate = {"lut": [], "real": [], "comp": [], "lut_harmon": [], 'mask': [], 'name': []}
    for step, batch in enumerate(val_loader):
        composite_image = batch['composite_image'].to(opt.device)
        real_image = batch['real_image'].to(opt.device)
        mask = batch['mask'].to(opt.device)
        category = batch['category']

        fg_INR_coordinates = batch['fg_INR_coordinates'].to(opt.device)

        "The number 20 here denotes the total frame number of one video clip in HYouTube."
        if step % 20 in checkpoint:
            with torch.no_grad():
                _, lut, _ = model(
                    composite_image,
                    mask,
                    fg_INR_coordinates,
                )
            all_intermediate['lut'].append(lut)

        all_intermediate['comp'].append(composite_image)
        all_intermediate['real'].append(real_image)
        all_intermediate['mask'].append(mask)
        all_intermediate['name'].append(batch['file_path'])

        if (step + 1) % 20 == 0:
            # We do the 3D LUT interpolation here.
            temp_lut = [select(checkpoint, x, all_intermediate['lut']) for x in range(20)]
            all_intermediate['lut'] = temp_lut
            for id in range(len(all_intermediate['lut'])):
                lut_transform_image = misc.normalize(torch.stack(
                    [TrilinearInterpolation(lut, image)[0] for lut, image in
                     zip(all_intermediate['lut'][id], misc.normalize(all_intermediate['comp'][id], opt, 'inv'))],
                    dim=0), opt)

                mask = all_intermediate['mask'][id]
                real_image = all_intermediate['real'][id]

                lut_transform_image = lut_transform_image * (mask > 100 / 255.) + real_image * (~(mask > 100 / 255.))

                # We here save the outputs as npy files. You can also save them as images by custom changes.
                # video, obj, img_number = all_intermediate['name'][id][0].replace('\\', '/').split('/')[-3:]
                #
                # result_name = os.path.join(
                #     "./HYouTube_Result",
                #     video + '_' + obj + '_' + img_number[:-3] + 'npy')
                # np.save(result_name, misc.normalize(lut_transform_image, opt, 'inv').detach().cpu().numpy())

                lut_mse, lut_fmse, lut_psnr, lut_ssim = metrics.calc_metrics(
                    misc.normalize(lut_transform_image, opt, 'inv'),
                    misc.normalize(real_image, opt, 'inv'), mask)

                for idx in range(len(category)):
                    lut_metric_log[category[idx]]['Samples'] += 1
                    lut_metric_log[category[idx]]['MSE'] += lut_mse[idx]
                    lut_metric_log[category[idx]]['fMSE'] += lut_fmse[idx]
                    lut_metric_log[category[idx]]['PSNR'] += lut_psnr[idx]
                    lut_metric_log[category[idx]]['SSIM'] += lut_ssim[idx]

                    lut_metric_log['All']['Samples'] += 1
                    lut_metric_log['All']['MSE'] += lut_mse[idx]
                    lut_metric_log['All']['fMSE'] += lut_fmse[idx]
                    lut_metric_log['All']['PSNR'] += lut_psnr[idx]
                    lut_metric_log['All']['SSIM'] += lut_ssim[idx]

                if (step + 1) / len(val_loader) * 100 >= current_process:
                    logger.info(f'Processing: {current_process}')
                    current_process += 10
            all_intermediate = {"lut": [], "real": [], "comp": [], "lut_harmon": [], 'mask': [], 'name': []}

    logger.info('=========================')
    for key in metric_log.keys():
        if opt.INRDecode:
            msg = f"{key}-'LUT_MSE': {lut_metric_log[key]['MSE'] / lut_metric_log[key]['Samples']:.2f}\n" \
                  f"{key}-'LUT_fMSE': {lut_metric_log[key]['fMSE'] / lut_metric_log[key]['Samples']:.2f}\n" \
                  f"{key}-'LUT_PSNR': {lut_metric_log[key]['PSNR'] / lut_metric_log[key]['Samples']:.2f}\n" \
                  f"{key}-'LUT_SSIM': {lut_metric_log[key]['SSIM'] / lut_metric_log[key]['Samples']:.4f}\n"
        else:
            msg = f"{key}-'LUT_MSE': {lut_metric_log[key]['MSE'] / lut_metric_log[key]['Samples']:.2f}\n" \
                  f"{key}-'LUT_fMSE': {lut_metric_log[key]['fMSE'] / lut_metric_log[key]['Samples']:.2f}\n" \
                  f"{key}-'LUT_PSNR': {lut_metric_log[key]['PSNR'] / lut_metric_log[key]['Samples']:.2f}\n" \
                  f"{key}-'LUT_SSIM': {lut_metric_log[key]['SSIM'] / lut_metric_log[key]['Samples']:.4f}\n"

        logger.info(msg)

    logger.info('=========================')


def main_process(opt):
    logger = misc.create_logger(os.path.join(opt.save_path, "log.txt"))
    cudnn.benchmark = True

    valset_path = os.path.join(opt.dataset_path, "test_list.txt")

    opt.transform_mean = [.5, .5, .5]
    opt.transform_var = [.5, .5, .5]
    torch_transform = transforms.Compose([transforms.ToTensor(),
                                          transforms.Normalize(opt.transform_mean, opt.transform_var)])

    valset_alb_transform = albumentations.Compose([Resize(opt.input_size, opt.input_size)],
                                                  additional_targets={'real_image': 'image', 'object_mask': 'image'})

    valset = dataset_generator(valset_path, valset_alb_transform, torch_transform, opt, mode='Val')

    val_loader = DataLoader(valset, opt.batch_size, shuffle=False, drop_last=False, pin_memory=True,
                            num_workers=opt.workers, persistent_workers=True)

    model = build_model(opt).to(opt.device)
    logger.info(f"Load pretrained weight from {opt.pretrained}")

    load_dict = torch.load(opt.pretrained)['model']
    for k in load_dict.keys():
        if k not in model.state_dict().keys():
            print(f"Skip {k}")
    model.load_state_dict(load_dict, strict=False)

    inference(val_loader, model, logger, opt)


if __name__ == '__main__':
    opt = parse_args()
    os.makedirs(opt.save_path, exist_ok=True)
    main_process(opt)